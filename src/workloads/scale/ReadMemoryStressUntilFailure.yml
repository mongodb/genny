SchemaVersion: 2018-07-01
Owner: Service Architecture
Description: |
  Used to test performance of the server under heavy memory pressure caused by read
  operations. This workload is expected to fail due to host(s) being unreachable as a
  result of mongod(s) running out of memory.

  To achieve this, documents are inserted in the form of {a: <id>, b: <random number>,
  c: <fill 520 KB>}. Many threads are then spawned to run aggregate sort on a number of
  documents. The document size and the number of documents to sort by each thread are
  tuned to be just under 100 MB, which is the memory limit for operations before
  spilling to disk. The threads would therefore each cause a close-to-max amount of
  memory to be used. Increasing the number of threads should cause the host(s) that
  process the operations to fail due to out-of-memory errors.

Keywords:
- scale
- memory stress
- aggregate
- sort
- insert
- fail
- oom
- out of memory

GlobalDefaults:
  dbname: &DBName memorystress
  MaxPhases: &MaxPhases 20

  # Many of these values are either arbitrary or multiples of other variables. It is only
  # important to keep the DocSize and SortStep (number of documents to sort) close to 100 MB
  # so each thread can use as much memory as possible when sorting.

  LoadThreads: &LoadThreads 128
  LoadBatchSize: &LoadBatchSize 500

  NumberOfDocuments: &NumDocs 20096
  # NumberOfDocumentsPerWorker = NumberOfDocuments / LoadThreads.
  NumberOfDocumentsPerWorker: &DocsPerThread 157
  # Used in Document: a to let the starting value of a = 0.
  DocumentStartAt: &DocStartAt -157
  DocSize: &DocSize 520000
  Document: &Doc
    # Document number/ID. Starts at *DocStartAt = -*DocsPerThread = -157 because ActorId
    # starts at 1, and therefore in order for the first output of ^Inc to be 0, "start"
    # would need to be offset by ActorId * *DocsPerThread = 157.
    a: {^Inc: {start: *DocStartAt, step: 1, multiplier: *DocsPerThread}}
    # Random number from [0, NumberOfDocuments].
    b: {^RandomInt: {min: 0, max: *NumDocs}}
    # Fill 520 KB.
    c: {^FastRandomString: {length: *DocSize}}

  # Number of documents to sort by each thread. Tuned so that *SortStep * *DocSize < 100 MB,
  # which is the maximum memory a sort query is allowed to use before spilling to disk.
  SortStep: &SortStep 180
  SortBatchSize: &SortBatchSize 180
  SortThreads: &SortThreads 250
  SortRepeat: &SortRepeat 5

Clients:
  Default:
    QueryOptions:
      socketTimeoutMS: -1
      maxPoolSize: 11000

Actors:
# Drop database to get rid of stale data. Useful when running locally multiple times.
- Name: Setup
  Type: RunCommand
  Threads: 1
  Phases:
    OnlyActiveInPhases:
      Active: [0]
      NopInPhasesUpTo: *MaxPhases
      PhaseConfig:
        Repeat: 1
        Database: *DBName
        Operations:
        - OperationName: RunCommand
          OperationCommand: {dropDatabase: 1}

# Profile everything, so we can see $currentOp executions stats. The workload will fail, analysis won't work.
- Name: LogLevel
  Type: RunCommand
  Threads: 1
  Phases:
    OnlyActiveInPhases:
      Active: [0]
      NopInPhasesUpTo: *MaxPhases
      PhaseConfig:
        Repeat: 1
        Database: admin
        Operations:
        - OperationName: RunCommand
          OperationCommand: { profile: 2 }

# Load 20,096 documents around 520KB as described by the structure in GlobalDefaults.
- Name: LoadDocuments
  Type: Loader
  Threads: *LoadThreads
  Phases:
    OnlyActiveInPhases:
      Active: [1]
      NopInPhasesUpTo: *MaxPhases
      PhaseConfig:
        CollectionCount: 1
        Database: *DBName
        Repeat: 1
        Document: *Doc
        MultipleThreadsPerCollection: true
        DocumentCount: *NumDocs
        BatchSize: *LoadBatchSize

# Create 50k idle cursors, to simulate a situation where we have some big memory consumers and many small ops.
- Name: CreateCursors
  Type: CrudActor
  Threads: 10000
  Phases:
    OnlyActiveInPhases:
      Active: [1]
      NopInPhasesUpTo: *MaxPhases
      PhaseConfig:
        Repeat: 1
        Database: someDb
        Collection: doesNotMatter
        ThrowOnFailure: false
        Operations:
        - OperationName: find
          OperationCommand:
            Filter: {}
            Options:
              BatchSize: 0 # We just want a cursor stablished, the collection does not contain data.

# Spawn many threads to sort enough documents to test server's capacity to handle memory pressure.
- Name: SortMany
  Type: RunCommand
  Threads: *SortThreads
  Phases:
    OnlyActiveInPhases:
      Active: [2]
      NopInPhasesUpTo: *MaxPhases
      PhaseConfig:
        Duration: 2 minutes
        # Repeat: *SortRepeat
        Database: *DBName
        ThrowOnFailure: false
        Operations:
        - OperationMetricsName: SortMany
          OperationName: RunCommand
          OperationCommand:
            # Loader default collection name.
            aggregate: Collection0
            # Sort *SortStep number of documents starting from a randomly chosen 'a'.
            pipeline:
              [{$match:
                {$expr:
                  {$let:
                    {vars:
                      {start: {^RandomInt: {min: 0, max: *NumDocs}}},
                    in: {$and: [$gte: ["$a", "$$start"], $lt: ["$a", {$add: ["$$start", *SortStep]}]]}
                    }
                  }
                }
              },
              {$sort: {b: 1}}]
            cursor: {batchSize: *SortBatchSize}

# - Name: currentOp
#   Type: AdminCommand
#   # Use a single thread. Multiple $currentOp will block due to the service context mutex (LockedClientCursor), which makes reported times meaningless.
#   Threads: 1
#   Phases:
#     OnlyActiveInPhases:
#       Active: [2]
#       NopInPhasesUpTo: *MaxPhases
#       PhaseConfig:
#         Duration: 2 minutes
#         Operations:
#         - OperationMetricsName: currentOp
#           OperationName: RunCommand
#           OperationCommand:
#             aggregate: 1
#             pipeline:
#               [{$currentOp: {allUsers: true, idleConnections: true, idleCursors: true, idleSessions: true}}, {$match: {type: "op"}}]
#             cursor: {}

# Force system.profile to be constantly flushed to disk
- Name: fsync
  Type: AdminCommand
  Threads: 1
  Phases:
    OnlyActiveInPhases:
      Active: [2]
      NopInPhasesUpTo: *MaxPhases
      PhaseConfig:
        Duration: 2 minutes
        Operations:
        - OperationMetricsName: fsync
          OperationName: RunCommand
          OperationCommand:
            fsync: 1

# Connect to mongodb server and run JS script
- Name: MongoshScriptRunnerWithDB
  Type: ExternalScriptRunner
  Threads: 1
  Phases:
      OnlyActiveInPhases:
        Active: [2]
        NopInPhasesUpTo: *MaxPhases
        PhaseConfig:
          Duration: 2 minutes
          Command: "mongo"
          MetricsName: ScriptMetrics
          Script: |
            const minutes = 4;
            const startTime = new Date().getTime();
            const targetEndTime = startTime + (1000 * 60 * minutes);
            function shouldStopTest() {
              return (new Date().getTime()) > targetEndTime;
            }

            function shouldShedOps() {
              let residentMemMb = db.serverStatus().mem.resident;
              //jsTestLog("Resident " + residentMemMb);
              return residentMemMb > 55000;
            }

            function compareOps(a, b) {
              return Date.parse(a.currentOpTime) - Date.parse(b.currentOpTime);
            }

            const testDB = db.getSiblingDB("memorystress");
            function killCursors(cursorArr) {
                //jsTestLog("Kill cursors " + cursorArr.length);
                testDB.runCommand( { killCursors: "Collection0", cursors: cursorArr } );
            }

            function loadShedding() {
              // Target only Collection0.
              //jsTestLog("Running currentOp");
              let pipeline = [{$currentOp: {allUsers: true, idleCursors: true}}, {$match: {ns: "memorystress.Collection0"}}];
              let currOpResult = db.aggregate(pipeline);

              // Do some sorting.
              //jsTestLog("sorting");
              let opsArr = currOpResult.toArray().sort(compareOps);
              //jsTestLog("shedding");

              let n = 0;
              let cursorArr = [];

              for (const op of opsArr) {
                //printjson(op);
                if(op.opid) {
                  db.killOp(op.opid);
                  n++;
                } else if(op?.cursor?.cursorId) {
                  cursorArr.push(op.cursor.cursorId);
                  n++;
                }

                if(cursorArr.length >= 20 || (cursorArr.length && op.opid)) {
                  killCursors(cursorArr);
                  cursorArr = [];
                }

                if((n%20==0) && !shouldShedOps()) {
                  sleep(1);
                  if(shouldShedOps()) {
                    n = 0;
                    continue;
                  }
                  jsTestLog("Break shedding");
                  cursorArr = [];
                  break;
                }
              }
              if(cursorArr.length) {
                killCursors(cursorArr);
              }
            }

            while(!shouldStopTest()) {
              if(shouldShedOps()) {
                loadShedding();
              }
            }









# Commented out because this should not be regularly scheduled, as the task is expected to fail.
# Uncomment the lines below (and possibly change the build variant) to run the workload.
# AutoRun:
# - When:
#     mongodb_setup:
#       $eq:
#       - shard-lite
