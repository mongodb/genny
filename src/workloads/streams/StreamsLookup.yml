SchemaVersion: 2018-07-01
Owner: Atlas Streams
Description: |
  Pipeline: Memory -> Lookup -> AddField -> Window (Group) -> Memory
  Input Documents: 8M
  BatchSize: 1k
  ForeignCollectionDocuments: 10k

  Simulates the scenario where incoming data needs to be merged with a foreign mongoDB collection
  and then propagated to a tumbling window which groups by a foreign column that was fetched
  from the $lookup (join) on the foreign mongoDB collection.

Keywords:
  - streams

GlobalDefaults:
  DatabaseName: &DatabaseName test
  TenantId: &TenantId test
  StreamProcessorName: &StreamProcessorName sp
  StreamProcessorId: &StreamProcessorId spid

  # Genny workload client typically has 16 CPUs, so use 16 inserter threads each inserting
  # 500 batches of 1k documents, so a total of 8M documents, which will all have unique keys
  # for the window so this will generate 8M keys on the open window.
  NumThreads: &NumThreads 16
  NumBatch1000xPerThread: &NumBatch1000xPerThread 50
  ExpectedDocumentCount: &ExpectedDocumentCount 800000 # NumThreads * NumBatch1000xPerThread * 1000
  NumAuctionDocuments: &NumAuctionDocuments 10000

  Channel: &Channel {^RandomInt: {min: 0, max: 10000}}
  Url: &Url {^FormatString: {format: "https://www.nexmark.com/%s/%s/%s/item.htm?query=1&channel_id=%d", withArgs: [
    {^RandomString: {length: {^RandomInt: {min: 3, max: 5}}}},
    {^RandomString: {length: {^RandomInt: {min: 3, max: 5}}}},
    {^RandomString: {length: {^RandomInt: {min: 3, max: 5}}}},
    *Channel
  ]}}

  AuctionCollectionName: &AuctionCollectionName auctions
  AuctionDocument: &AuctionDocument
    _id: {^Inc: {start: 1000}}
    impressionId: {^RandomString: {length: 16}}
    publisherId: {^RandomString: {length: 16}}
    sellerId: {^RandomString: {length: 16}}

  Document: &Document
    # There will only be `NumAuctionDocuments`, so generate a random auction ID b/w (1k, 1k + `NumAuctionDocuments`]
    auctionId: {^RandomInt: {min: 1000, max: 11000}}
    bidderId: {^Inc: {start: 1000, multipler: 1}}
    price: {^RandomDouble: {min: 100, max: 100000000}}
    channel: *Channel
    url: *Url
    dateTime: {^IncDate: {start: "2023-01-01T00:00:00.000", step: 10}}

  # Manually construct 1k batch rather than using ^Array function so that all documents in a
  # single 1k batch have the same timestamp. And since this is across 16 threads, each timestamp
  # will have (16 * 1000) documents, and with a `IncDate` step size of 10 milliseconds, each one
  # second window will have (16 * 1000 * 100) documents => 1,600,000 documents.
  Batch10x: &Batch10x [*Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document]
  Batch100x: &Batch100x {^FlattenOnce: [*Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x]}
  Batch1000x: &Batch1000x {^FlattenOnce: [
    *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x
  ]}

Actors:

  # Loads the `auction` collection which is what the `$lookup` operator in the streaming pipeline will
  # be joining incoming stream data with.
  - Name: Loader
    Type: CrudActor
    ClientName: Default
    Database: *DatabaseName
    Threads: 1
    Phases:
      - Phase: 0
        Repeat: *NumAuctionDocuments
        Collection: *AuctionCollectionName
        Operations:
          - OperationName: insertOne
            OperationCommand:
              Document: *AuctionDocument
      - Phase: 1..3
        Nop: true

  - Name: Setup
    Type: RunCommand
    ClientName: Stream
    Threads: 1
    Phases:
      - Phase: 0
        Nop: true
      - Phase: 1
        Repeat: 1
        Database: *DatabaseName
        Operations:
          - OperationMetricsName: CreateStreamProcessor
            OperationName: RunCommand
            ReportMetrics: false
            OperationCommand:
              streams_startStreamProcessor: ""
              tenantId: *TenantId
              name: *StreamProcessorName
              processorId: *StreamProcessorId
              pipeline: [
                {
                  $source: {
                    connectionName: "__testMemory",
                    timeField: {$convert: {input: "$dateTime", to: "date"}}
                  }
                },
                {
                  $lookup: {
                    from: {connectionName: "db", db: *DatabaseName, coll: *AuctionCollectionName},
                    localField: "auctionId",
                    foreignField: "_id",
                    as: "auctions"
                  }
                },
                {$addFields: {auction: {$first: "$auctions"}}},
                {
                  $tumblingWindow: {
                    interval: {size: 1, unit: "second"},
                    allowedLateness: {size: 1, unit: "second"},
                    pipeline: [{
                      $group: {
                        _id: "$auction.sellerId",
                        minPrice: {$min: "$price"},
                        maxPrice: {$max: "$price"},
                        sumPrice: {$sum: "$price"},
                        avgPrice: {$avg: "$price"}
                      }
                    }]
                  }
                },
                {$emit: {connectionName: "__noopSink"}}
              ]
              connections: [
                {name: "__testMemory", type: "in_memory", options: {}},
                {name: "db", type: "atlas", options: {uri: {^ClientURI: {Name: "Default"}}}}
              ]
              options: { featureFlags: {} }
      - Phase: 2
        Nop: true
      - Phase: 3
        Repeat: 1
        Database: *DatabaseName
        Operations:
          - OperationMetricsName: Stop
            OperationName: RunCommand
            ReportMetrics: false
            OperationCommand:
              streams_stopStreamProcessor: ""
              tenantId: *TenantId
              name: *StreamProcessorName
              processorId: *StreamProcessorId

  - Name: Insert_Batch1000x
    Type: RunCommand
    ClientName: Stream
    Threads: *NumThreads
    Phases:
      - Phase: 0..1
        Nop: true
      - Phase: 2
        Repeat: *NumBatch1000xPerThread
        Database: *DatabaseName
        Operations:
          - OperationMetricsName: Insert
            OperationName: RunCommand
            OperationCommand:
              streams_testOnlyInsert: ""
              tenantId: *TenantId
              name: *StreamProcessorName
              processorId: *StreamProcessorId
              documents: *Batch1000x
      - Phase: 3
        Nop: true

  - Name: Lookup.MemorySource.Stats
    Type: StreamStatsReporter
    ClientName: Stream
    Database: *DatabaseName
    Threads: 1
    Phases:
      - Phase: 0..1
        Nop: true
      - Phase: 2
        Repeat: 1
        TenantId: *TenantId
        StreamProcessorName: *StreamProcessorName
        StreamProcessorId: *StreamProcessorId
        ExpectedDocumentCount: *ExpectedDocumentCount
      - Phase: 3
        Nop: true
