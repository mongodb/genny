SchemaVersion: 2018-07-01
Owner: Atlas Streams
Description: |
  Pipeline: Memory -> Lookup -> AddField -> Window (Group) -> Memory
  Input Documents: 8M
  BatchSize: 1k
  ForeignCollectionDocuments: 10k

  Simulates the scenario where incoming data needs to be merged with a foreign mongoDB collection
  and then propagated to a tumbling window which groups by a foreign column that was fetched
  from the $lookup (join) on the foreign mongoDB collection.

Keywords:
- streams

GlobalDefaults:
  DatabaseName: &DatabaseName test
  StreamProcessorName: &StreamProcessorName sp

  # Genny workload client typically has 16 CPUs, so use 16 inserter threads each inserting
  # 500 batches of 1k documents, so a total of 8M documents, which will all have unique keys
  # for the window so this will generate 8M keys on the open window.
  NumThreads: &NumThreads 16
  NumBatch1000xPerThread: &NumBatch1000xPerThread 50
  ExpectedDocumentCount: &ExpectedDocumentCount 800000  # NumThreads * NumBatch1000xPerThread * 1000
  NumAuctionDocuments: &NumAuctionDocuments 10000

  Channel: &Channel {^RandomInt: { min: 0, max: 10000 }}
  Url: &Url {^FormatString: { format: "https://www.nexmark.com/%s/%s/%s/item.htm?query=1&channel_id=%d", withArgs: [
    {^RandomString: { length: {^RandomInt: { min: 3, max: 5 }}}},
    {^RandomString: { length: {^RandomInt: { min: 3, max: 5 }}}},
    {^RandomString: { length: {^RandomInt: { min: 3, max: 5 }}}},
    *Channel
  ]}}

  AuctionCollectionName: &AuctionCollectionName auctions
  AuctionDocument: &AuctionDocument
    _id: {^Inc: { start: 1000 }}
    impressionId: {^RandomString: { length: 16 }}
    publisherId: {^RandomString: { length: 16 }}
    sellerId: {^RandomString: { length: 16 }}

  Document: &Document
    # There will only be `NumAuctionDocuments`, so generate a random auction ID b/w (1k, 1k + `NumAuctionDocuments`]
    auctionId: {^RandomInt: { min: 1000, max: 11000 }}
    bidderId: {^Inc: { start: 1000, multipler: 1 }}
    price: {^RandomDouble: { min: 100, max: 100000000 }}
    channel: *Channel
    url: *Url
    dateTime: {^IncDate: { start: "2023-01-01T00:00:00.000", step: 10 }}

  # Manually construct 1k batch rather than using ^Array function so that all documents in a
  # single 1k batch have the same timestamp. And since this is across 16 threads, each timestamp
  # will have (16 * 1000) documents, and with a `IncDate` step size of 10 milliseconds, each one
  # second window will have (16 * 1000 * 100) documents => 1,600,000 documents.
  Batch10x: &Batch10x [*Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document]
  Batch100x: &Batch100x {^FlattenOnce: [*Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x]}
  Batch1000x: &Batch1000x {^FlattenOnce: [*Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x]}

Actors:

# Loads the `auction` collection which is what the `$lookup` operator in the streaming pipeline will
# be joining incoming stream data with.
- Name: Loader
  Type: CrudActor
  ClientName: Default
  Database: *DatabaseName
  Threads: 1
  Phases:
  - Phase: 0
    Repeat: *NumAuctionDocuments
    Collection: *AuctionCollectionName
    Operations:
    - OperationName: insertOne
      OperationCommand:
        Document: *AuctionDocument
  - Phase: 1..3
    Nop: true

- Name: Setup
  Type: RunCommand
  ClientName: Stream
  Threads: 1
  Phases:
  - Phase: 0
    Nop: true
  - Phase: 1
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: CreateStreamProcessor
      OperationName: RunCommand
      OperationCommand:
        streams_startStreamProcessor: ""
        name: *StreamProcessorName
        pipeline: [
          {
            $source: {
              connectionName: "__testMemory",
              timeField: { $convert: { input: "$dateTime", to: "date" }},
            }
          },
          {
            $lookup: {
              from: {
                connectionName: "db",
                db: *DatabaseName,
                coll: *AuctionCollectionName
              },
              localField: "auctionId",
              foreignField: "_id",
              as: "auctions"
            }
          },
          {
            $addFields: {
              auction: { $first: "$auctions" }
            }
          },
          {
            $tumblingWindow: {
              interval: { size: 1, unit: "second" },
              allowedLateness: { size: 1, unit: "second" },
              pipeline: [
                {
                  $group: {
                    _id: "$auction.sellerId",
                    minPrice: { $min: "$price" },
                    maxPrice: { $max: "$price" },
                    sumPrice: { $sum: "$price" },
                    avgPrice: { $avg: "$price" }
                  }
                }
              ]
            }
          },
          { $emit: { connectionName: "__noopSink" } }
        ]
        connections: [
          { name: "__testMemory", type: "in_memory", options: {} },
          { name: "db", type: "atlas", options: { uri: {^ClientURI: { Name: "Default" }} }}
        ]
        options: {}
  - Phase: 2
    Nop: true
  - Phase: 3
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: Stop
      OperationName: RunCommand
      OperationCommand:
        streams_stopStreamProcessor: ""
        name: *StreamProcessorName

- Name: Insert_Batch1000x
  Type: RunCommand
  ClientName: Stream
  Threads: *NumThreads
  Phases:
  - Phase: 0..1
    Nop: true
  - Phase: 2
    Repeat: *NumBatch1000xPerThread
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: Insert
      OperationName: RunCommand
      OperationCommand:
        streams_testOnlyInsert: ""
        name: *StreamProcessorName
        documents: *Batch1000x
  - Phase: 3
    Nop: true

- Name: Lookup.MemorySource.Stats
  Type: StreamStatsReporter
  ClientName: Stream
  Database: *DatabaseName
  Threads: 1
  Phases:
  - Phase: 0..1
    Nop: true
  - Phase: 2
    Repeat: 1
    StreamProcessorName: *StreamProcessorName
    ExpectedDocumentCount: *ExpectedDocumentCount
  - Phase: 3
    Nop: true
