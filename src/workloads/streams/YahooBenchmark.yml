SchemaVersion: 2018-07-01
Owner: "@10gen/altas-streams"
Description: |
  This is the yahoo streaming benchmark which is based on https://developer.yahoo.com/blogs/135370591481/.
  This is meant to test a typical real world case for streaming pipelines, which will be a pipeline that
  filters then groups incoming data into timed windows.

  This differentiates from the original workload because it doesn't actual do joins with data in
  redis. The streaming pipeline is set up to track the number of views per campaign for every 10s
  tumbling window, while projecting only the campaign ID, ad ID, and event timestamp to the sink.
  There are 10 ads per campaign and 100 campaigns in total.

Keywords:
- streams

GlobalDefaults:
  DatabaseName: &DatabaseName "test"
  RandomUUID: &RandomUUID {^RandomString: { length: 16 }}
  StreamProcessorName: &StreamProcessorName "sp"

  AdIds: &AdIds             {^Cycle: { ofLength: 1000, fromGenerator: *RandomUUID }}
  CampaignIds: &CampaignIds {^Cycle: { ofLength: 100, fromGenerator: *RandomUUID }}

  Document: &Document
    userId:     *RandomUUID
    adId:       *AdIds
    campaignId: *CampaignIds
    pageId:     *RandomUUID
    adType:     {^Choose: { from: ["banner", "modal", "sponsored-search", "mail", "mobile"] }}
    eventType:  {^Choose: { from: ["view", "click", "purchase"] }}
    eventTime:  {^IncDate: { start: "2023-01-01T00:00:00.000", step: 10 }}
    ipAddress:  {^IP: {}}

  Batch1000x: &Batch1000x {^Array: { number: 1000, of: *Document }}

Actors:
- Name: Setup
  Type: RunCommand
  Threads: 1
  Phases:
  - Phase: 0
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: CreateStreamProcessor
      OperationName: RunCommand
      OperationCommand:
        streams_startStreamProcessor: ""
        name: *StreamProcessorName
        pipeline: [
          { 
            $source: { 
              connectionName: "kafka",
              topic: "topic-input",
              timeField: { $convert: { input: "$eventTime", to: "date" }},
              allowedLateness: { size: 10, unit: "second" },
              testOnlyPartitionCount: 1
            }
          },
          { $match: { eventType: "view" } },
          { $project: { adId: 1, campaignId: 1, eventTime: 1 }},
          { 
            $tumblingWindow: {
              interval: { size: 10, unit: "second" },
              pipeline: [
                { $group: {
                    _id: "$campaignId",
                    count: { $count: {} },
                }}
              ]
            }
          },
          { $emit: { connectionName: "__testMemory" } }
        ]
        connections: [
          {
            name: "kafka",
            type: "kafka",
            options: {
              bootstrapServers: "localhost:9092",
              isTestKafka: true
            }
          },
          { name: "__testMemory", type: "in_memory", options: {} }
        ]
  - Phase: 1
    Nop: true
  - Phase: 2
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: Stop
      OperationName: RunCommand
      OperationCommand:
        streams_stopStreamProcessor: ""
        name: *StreamProcessorName

- Name: Insert_Batch1000x
  Type: RunCommand
  Threads: 32
  Phases:
    - Phase: 0
      Nop: true
    - Phase: 1
      Duration: 1 minute
      Database: *DatabaseName
      Operations:
      - OperationMetricsName: Insert
        OperationName: RunCommand
        OperationCommand:
          streams_testOnlyInsert: ""
          name: *StreamProcessorName
          documents: *Batch1000x
    - Phase: 2
      Nop: true

AutoRun:
- When:
    mongodb_setup:
      $eq:
      - standalone-streams
    branch_name:
      $gte: v7.2
