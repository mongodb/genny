SchemaVersion: 2018-07-01
Owner: "@10gen/altas-streams"
Description: |
  This is the yahoo streaming benchmark which is based on https://developer.yahoo.com/blogs/135370591481/.
  This is meant to test a typical real world case for streaming pipelines, which will be a pipeline that
  filters then groups incoming data into timed windows.

  This differentiates from the original workload because it doesn't actual do joins with data in
  redis. The streaming pipeline is set up to track the number of views per campaign for every 10s
  tumbling window, while projecting only the campaign ID, ad ID, and event timestamp to the sink.
  There are 10 ads per campaign and 100 campaigns in total.

Keywords:
- streams

GlobalDefaults:
  DatabaseName: &DatabaseName "test"
  RandomUUID: &RandomUUID {^RandomString: { length: 16 }}
  StreamProcessorName: &StreamProcessorName "sp"

  # Use 16 inserter threads that insert 100 batches of 1k documents, so
  # total of 1.6M documents.
  NumThreads: &NumThreads 16
  NumBatches: &NumBatches 1000

  AdIds: &AdIds             {^Cycle: { ofLength: 1000, fromGenerator: *RandomUUID }}
  CampaignIds: &CampaignIds {^Cycle: { ofLength: 100, fromGenerator: *RandomUUID }}

  Document: &Document
    userId:     *RandomUUID
    adId:       *AdIds
    campaignId: *CampaignIds
    pageId:     *RandomUUID
    adType:     {^Choose: { from: ["banner", "modal", "sponsored-search", "mail", "mobile"] }}
    eventType:  {^Choose: { from: ["view", "click", "purchase"] }}
    eventTime:  {^IncDate: { start: "2023-01-01T00:00:00.000", step: 10 }}
    ipAddress:  {^IP: {}}

  Batch10x: &Batch10x [*Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document]
  Batch100x: &Batch100x {^FlattenOnce: [*Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x]}

Actors:
- Name: Setup
  Type: RunCommand
  ClientName: Stream
  Threads: 1
  Phases:
  - Phase: 0
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: CreateStreamProcessor
      OperationName: RunCommand
      OperationCommand:
        streams_startStreamProcessor: ""
        name: *StreamProcessorName
        pipeline: [
          { 
            $source: { 
              connectionName: "kafka",
              topic: "topic-input",
              timeField: { $convert: { input: "$eventTime", to: "date" }},
              allowedLateness: { size: 10, unit: "second" },
              testOnlyPartitionCount: 1
            }
          },
          { $match: { eventType: "view" } },
          { $project: { adId: 1, campaignId: 1, eventTime: 1 }},
          { 
            $tumblingWindow: {
              interval: { size: 10, unit: "second" },
              pipeline: [
                { $group: {
                    _id: "$campaignId",
                    count: { $count: {} },
                }}
              ]
            }
          },
          { $emit: { connectionName: "__testMemory" } }
        ]
        connections: [
          {
            name: "kafka",
            type: "kafka",
            options: {
              bootstrapServers: "localhost:9092",
              isTestKafka: true
            }
          },
          { name: "__testMemory", type: "in_memory", options: {} }
        ]
  - Phase: 1
    Nop: true
  - Phase: 2
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: Stop
      OperationName: RunCommand
      OperationCommand:
        streams_stopStreamProcessor: ""
        name: *StreamProcessorName

- Name: Insert_Batch100x
  Type: RunCommand
  ClientName: Stream
  Threads: *NumThreads
  Phases:
    - Phase: 0
      Nop: true
    - Phase: 1
      Repeat: *NumBatches
      Database: *DatabaseName
      Operations:
      - OperationMetricsName: Insert
        OperationName: RunCommand
        OperationCommand:
          streams_testOnlyInsert: ""
          name: *StreamProcessorName
          documents: *Batch100x
    - Phase: 2
      Nop: true

- Name: Stats
  Type: StreamStatsReporter
  Database: *DatabaseName
  Threads: 1
  Phases:
  - Phase: 0
    Nop: true
  - Phase: 1
    Repeat: 1
    StreamProcessorName: *StreamProcessorName
    ExpectedDocumentCount: 1600000
  - Phase: 2
    Nop: true

AutoRun:
- When:
    mongodb_setup:
      $eq:
      - standalone-streams
    branch_name:
      $gte: v7.2
