SchemaVersion: 2018-07-01
Owner: "@10gen/altas-streams"
Description: |
  Workload that generates 1.6M documents all with unique keys that are inserted
  into a tumbling window streaming pipeline with the same timestamp so that they
  fall into the same window.

Keywords:
- streams

GlobalDefaults:
  DatabaseName: &DatabaseName test
  StreamProcessorName: &StreamProcessorName sp

  Channel: &Channel {^RandomInt: { min: 0, max: 10000 }}
  Url: &Url {^FormatString: { format: "https://www.nexmark.com/%s/%s/%s/item.htm?query=1&channel_id=%d", withArgs: [
    {^RandomString: { length: {^RandomInt: { min: 3, max: 5 }}}},
    {^RandomString: { length: {^RandomInt: { min: 3, max: 5 }}}},
    {^RandomString: { length: {^RandomInt: { min: 3, max: 5 }}}},
    *Channel
  ]}}

  Document: &Document
    auction: {^Inc: { start: 1000 }}
    bidder: {^Inc: { start: 1000 }}
    price: {^RandomDouble: {min: 100, max: 100000000}}
    channel: *Channel
    url: *Url
    dateTime: "2023-01-01T00:00:00.000"

  Batch10x: &Batch10x [*Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document, *Document]
  Batch100x: &Batch100x {^FlattenOnce: [*Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x, *Batch10x]}
  Batch1000x: &Batch1000x {^FlattenOnce: [*Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x, *Batch100x]}

  # Genny workload client typically has 16 CPUs, so use 16 inserter threads each inserting
  # 100 batches of 1k documents, so total of 1.6M documents.
  NumThreads: &NumThreads 16

Actors:
- Name: Setup
  Type: RunCommand
  ClientName: Stream
  Threads: 1
  Phases:
  - Phase: 0
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: CreateStreamProcessor
      OperationName: RunCommand
      OperationCommand:
        streams_startStreamProcessor: ""
        name: *StreamProcessorName
        pipeline: [
          {
            $source: {
              connectionName: "__testMemory",
              timeField: { $convert: { input: "$dateTime", to: "date" }},
              allowedLateness: { size: 1, unit: "second" },
            }
          },
          {
            $tumblingWindow: {
              interval: { size: 1, unit: "second" },
              pipeline: [
                { 
                  $group: {
                    _id: {
                      auction: "$auction",
                      url: "$url"
                    },
                    minPrice: { $min: "$price" }
                    maxPrice: { $max: "$price" }
                    sumPrice: { $sum: "$price" }
                    avgPrice: { $avg: "$price" }
                  }
                }
              ]
            }
          },
          { $emit: { connectionName: "__testMemory" } }
        ]
        connections: [{ name: "__testMemory", type: "in_memory", options: {} }]
  - Phase: 1
    Nop: true
  - Phase: 2
    Repeat: 1
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: Stop
      OperationName: RunCommand
      OperationCommand:
        streams_stopStreamProcessor: ""
        name: *StreamProcessorName

- Name: Insert_Batch1000x
  Type: RunCommand
  ClientName: Stream
  Threads: *NumThreads
  Phases:
  - Phase: 0
    Nop: true
  - Phase: 1
    Repeat: 10
    Database: *DatabaseName
    Operations:
    - OperationMetricsName: Insert
      OperationName: RunCommand
      OperationCommand:
        streams_testOnlyInsert: ""
        name: *StreamProcessorName
        documents: *Batch1000x
  - Phase: 2
    Nop: true

- Name: Stats
  Type: StreamStatsReporter
  Database: *DatabaseName
  ClientName: Stream
  Threads: 1
  Phases:
  - Phase: 0
    Nop: true
  - Phase: 1
    Repeat: 1
    StreamProcessorName: *StreamProcessorName
    ExpectedDocumentCount: 160000
  - Phase: 2
    Nop: true

AutoRun:
- When:
    mongodb_setup:
      $eq:
      - standalone-streams
    branch_name:
      $gte: v7.2
